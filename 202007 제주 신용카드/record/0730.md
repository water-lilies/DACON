## Ensemble model for Jeju Dacon

> 2020. 07.30



- CatBoost

- LGB

- XGB

  ==> 모델을 각각 저장하여 앙상블



### Code

```python
from catboost import CatBoostRegressor

cb_model = CatBoostRegressor(iterations=5000,           # 반복횟수
                           learning_rate=0.1,           # 러닝레이트
                           l2_leaf_reg=3.5,             # L2 정규화 계수
                           depth=8,                     # 트리 깊이
                           rsm=0.98,                    # Random subspace method
                           metric_period=1000,
                           loss_function= 'MultiRMSE',      
                           eval_metric= 'MultiRMSE',     # 성능 측정
                           use_best_model=True,
                           random_seed=42                # 랜덤시드 고정
                          ) 

```



```python
import lightgbm as lgb

train_ds = lgb.Dataset(x_train, label=y_train)
val_ds = lgb.Dataset(x_val, label=y_val)

def rmsle_1(y_pred, data):
    y_true = np.array(data.get_label())
    score= np.sqrt(np.square(np.log1p(y_pred + 1) - np.log1p(y_true + 1)).mean())
    return 'rmsle', score, False

params = {
            'max_depth': -1,
            'learning_rate' : 0.05,
            'boosting_type': 'gbdt',
            'objective': 'tweedie',
            'tweedie_variance_power': 1.1,
            'metric': 'custom',
            'sub_row' : 0.75,
            'lambda_l2' : 0.1
        }

lgb_model = lgb.train(params,
                   train_ds,
                   20000,
                   val_ds,
                   verbose_eval = 1000,
                   early_stopping_rounds = 1000,
                   feval = rmsle_1
                 )
```



```python
import xgboost as xgb

train_ds = xgb.DMatrix(x_train, label=y_train)
val_ds = xgb.DMatrix(x_val, label=y_val)
watchlist = [(val_ds, 'eval'), (train_ds, 'train')]

param = {
            'booster': 'gbtree',
            'max_depth': 8,
            'objective': 'reg:squarederror',  # objective: loss func. 정의.
            'eta': 0.01,                      # learning rate(학습률) X의 움직임
            'colsample_bytree': 0.8,
            'colsample_bylevel': 0.9,
            'seed': 10
        }

xgb_model = xgb.train(params,
                  train_ds,
                  num_boost_round = 2500,
                  early_stopping_rounds = 1000,
                  verbose_eval = 1000,
                  evals = watchlist 
                  )
```



### Train

```
cb_model

bestTest = 0.8158041866
bestIteration = 4999


lgb_model

Did not meet early stopping. Best iteration is:
[20000]	valid_0's rmsle: 0.0504011


xgb_model

[2499]	eval-rmse:0.81952	train-rmse:0.79730
```



### Ensemble model

```python
cb_pred = cb_model.predict(temp)
lgb_pred = lgb_model.predict(temp)
xgb_pred = xgb_model.predict(temp)

ensemble = cb_pred*0.25 + lgb_pred*0.5 + xgb_pred*0.25
```


