{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3f8ba5c89b74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from simulator import Simulator\n",
    "\n",
    "class FactoryEnv(gym.Env):\n",
    "    def __init__(self, is_train):\n",
    "        self.is_train = is_train\n",
    "        self.simulator = Simulator()\n",
    "\n",
    "        self.order_data = pd.read_csv(\"data/order.csv\")\n",
    "        for i in range(40):\n",
    "            self.order_data.loc[91+i,:] = ['0000-00-00', 0, 0, 0, 0]        \n",
    "\n",
    "        self.submission = pd.read_csv(\"data/sample_submission.csv\")\n",
    "    \n",
    "        self.work_time = [28, 98] * 17 + [42]\n",
    "        self.action_plus = [(0.0, 0.0), (5.8, 0.0), (0.0, 5.8), (5.8, 5.8)]\n",
    "\n",
    "        self.MOL_queue = np.zeros([49, 4])\n",
    "\n",
    "    def save_csv(self):\n",
    "        PRTs = self.submission[[\"PRT_1\", \"PRT_2\", \"PRT_3\", \"PRT_4\"]].values\n",
    "        PRTs = (PRTs[:-1]-PRTs[1:])[24*23:]\n",
    "        PRTs[-1] = [0., 0., 0., 0.]\n",
    "        PRTs = np.ceil(PRTs * 1.1)+1\n",
    "        PAD = np.zeros((24*23+1, 4))\n",
    "        PRTs = np.append(PRTs, PAD, axis=0).astype(int)\n",
    "        self.submission.loc[:, \"PRT_1\":\"PRT_4\"] = PRTs\n",
    "\n",
    "        self.submission.to_csv(\"test.csv\", index=False)\n",
    "\n",
    "    def reset(self):\n",
    "        self.now_stock = np.array(pd.read_csv(\"data/stock.csv\"), dtype=np.float32)[0]\n",
    "\n",
    "        self.step_count = 0\n",
    "        self.work_index = 0\n",
    "        self.remain_time = 0\n",
    "\n",
    "        self.line_A_yield = 0.0\n",
    "        self.line_B_yield = 0.0\n",
    "\n",
    "        self.line_A_MOL = []\n",
    "        self.line_B_MOL = []\n",
    "\n",
    "        state = np.concatenate([[self.step_count], [0]*20, self.now_stock[8:]])/1000000\n",
    "\n",
    "        return state\n",
    "\n",
    "    def step1(self, action):\n",
    "        action_list = [(1, 1), (2, 2), (3, 3), (4, 4), (1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)]\n",
    "        \n",
    "        self.line_A_MOL.append(action_list[action][0])\n",
    "        self.line_B_MOL.append(action_list[action][1])\n",
    "        \n",
    "    def step2(self, action):\n",
    "        if self.remain_time == 0:\n",
    "            self.remain_time = self.work_time[self.work_index] - 1\n",
    "            self.work_index += 1\n",
    "        else:\n",
    "            self.remain_time -= 1\n",
    "\n",
    "        if self.step_count == 552:\n",
    "            self.line_A_yield = 3.2\n",
    "            self.line_B_yield = 3.2\n",
    "\n",
    "        def process():\n",
    "            self.now_stock[4:8] += self.MOL_queue[0]\n",
    "            if self.step_count > 551:\n",
    "                self.MOL_queue[-1][self.line_A_MOL[math.floor((self.work_index-1)/2)]-1] = self.line_A_yield * 0.975\n",
    "                self.MOL_queue[-1][self.line_B_MOL[math.floor((self.work_index-1)/2)]-1] = self.line_B_yield * 0.975\n",
    "\n",
    "            self.MOL_queue[:-1] = self.MOL_queue[1:]\n",
    "            self.MOL_queue[-1] = [0, 0, 0, 0]\n",
    "\n",
    "            if self.step_count > 551:\n",
    "                self.now_stock[self.line_A_MOL[math.floor((self.work_index-1)/2)]-1] -= self.line_A_yield\n",
    "                self.now_stock[self.line_B_MOL[math.floor((self.work_index-1)/2)]-1] -= self.line_B_yield\n",
    "            \n",
    "        if self.work_index % 2 == 0:\n",
    "            self.line_A_yield = self.action_plus[action][0]\n",
    "            self.line_B_yield = self.action_plus[action][1]\n",
    "\n",
    "            process()\n",
    "\n",
    "        self.submission.loc[self.step_count, \"PRT_1\":\"PRT_4\"] = self.now_stock[:4]\n",
    "        \n",
    "        # done, reward\n",
    "        if self.step_count == 2183:\n",
    "            done = True\n",
    "            score, _ = self.simulator.get_score(self.submission)\n",
    "            reward = (20000000 - score) / 20000000\n",
    "            print(f\"reward : {reward}\")\n",
    "        else: \n",
    "            reward = 0\n",
    "            done = False\n",
    "\n",
    "        # write\n",
    "        if self.work_index % 2 != 0:\n",
    "            self.submission.loc[self.step_count, \"Event_A\"] = f\"CHECK_{self.line_A_MOL[math.floor((self.work_index-1)/2)]}\"\n",
    "            self.submission.loc[self.step_count, \"MOL_A\"] = 0.0\n",
    "            self.submission.loc[self.step_count, \"Event_B\"] = f\"CHECK_{self.line_B_MOL[math.floor((self.work_index-1)/2)]}\"\n",
    "            self.submission.loc[self.step_count, \"MOL_B\"] = 0.0\n",
    "        else:\n",
    "            self.submission.loc[self.step_count, \"Event_A\"] = \"PROCESS\"\n",
    "            self.submission.loc[self.step_count, \"Event_B\"] = \"PROCESS\"\n",
    "            if self.step_count > 551:\n",
    "                self.submission.loc[self.step_count, \"MOL_A\"] = round(self.line_A_yield, 1)\n",
    "                self.submission.loc[self.step_count, \"MOL_B\"] = round(self.line_B_yield, 1)\n",
    "            else:\n",
    "                self.submission.loc[self.step_count, \"MOL_A\"] = 0.\n",
    "                self.submission.loc[self.step_count, \"MOL_B\"] = 0.\n",
    " \n",
    "        # state t+1\n",
    "        self.step_count += 1\n",
    "        state = np.concatenate([[self.step_count], np.array(self.order_data.loc[self.step_count//24:(self.step_count//24+4), 'BLK_1':'BLK_4']).reshape(-1), self.now_stock[8:]+400*np.sum(self.MOL_queue, axis=0)+self.now_stock[4:8]*400])/1000000\n",
    "\n",
    "        info = {}            \n",
    "        return state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import signal\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "clip_range = 0.2\n",
    "gamma = 0.99\n",
    "lam = 0.95\n",
    "learning_rate = 0.001\n",
    "\n",
    "hidden_size = 256\n",
    "\n",
    "HORIZON = 2184\n",
    "train_iter = 2\n",
    "\n",
    "save_interval = 5\n",
    "\n",
    "# True -> train\n",
    "# False -> inference\n",
    "is_train = True\n",
    "\n",
    "\n",
    "class GracefulKiller:\n",
    "    def __init__(self):\n",
    "        self.kill_now = False\n",
    "        signal.signal(signal.SIGINT, self.exit_gracefully)\n",
    "        signal.signal(signal.SIGTERM, self.exit_gracefully)\n",
    "\n",
    "    def exit_gracefully(self, signum, frame):\n",
    "        self.kill_now = True\n",
    "\n",
    "\n",
    "class PPO(nn.Module):\n",
    "    def __init__(self, output_shape):\n",
    "        super(PPO, self).__init__()\n",
    "        self.buffer = []\n",
    "        input_shape = 1 + 20 + 4\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_shape, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.logits_net = nn.Linear(hidden_size, output_shape)\n",
    "        self.v_net  = nn.Linear(hidden_size, 1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def pi(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.logits_net(x)\n",
    "        return Categorical(logits=x)\n",
    "    \n",
    "    def v(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        v = self.v_net(x)\n",
    "        return v\n",
    "      \n",
    "    def store(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "        \n",
    "    def update(self):\n",
    "        states = torch.tensor([e[0] for e in self.buffer], dtype=torch.float)\n",
    "        actions = torch.tensor([[e[1]] for e in self.buffer])\n",
    "        rewards = torch.tensor([[e[2]] for e in self.buffer], dtype=torch.float)\n",
    "        next_states = torch.tensor([e[3] for e in self.buffer], dtype=torch.float)\n",
    "        probs = torch.tensor([[e[4]] for e in self.buffer], dtype=torch.float)\n",
    "        dones = torch.tensor([[1-e[5]] for e in self.buffer])\n",
    "        self.buffer = []\n",
    "\n",
    "        for _ in range(train_iter):\n",
    "            td_target = rewards + gamma * self.v(next_states) * dones\n",
    "            delta = td_target - self.v(states)\n",
    "            delta = delta.detach().numpy()\n",
    "\n",
    "            advantages = []\n",
    "            advantage = 0.0\n",
    "            for delta_t in delta[::-1]:\n",
    "                advantage = gamma * lam * advantage + delta_t[0]\n",
    "                advantages.append([advantage])\n",
    "            advantages.reverse()\n",
    "            advs = torch.tensor(advantages, dtype=torch.float)\n",
    "\n",
    "            pi = self.pi(states)\n",
    "            ratio = torch.exp(pi.log_prob(actions) - torch.log(probs))  \n",
    "            cilp_ratio = torch.clamp(ratio, 1-clip_range, 1+clip_range)\n",
    "\n",
    "            pi_loss = -torch.mean(torch.min(ratio*advs, cilp_ratio*advs))\n",
    "            vf_loss = torch.mean(torch.pow(self.v(states) - td_target.detach(), 2))\n",
    "\n",
    "            loss = pi_loss + vf_loss\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "\n",
    "def main():\n",
    "    env = FactoryEnv(is_train)\n",
    "    killer = GracefulKiller()\n",
    "\n",
    "    model1 = PPO(10)\n",
    "    model2 = PPO(4)\n",
    "    if os.path.exists(\"save.pt\"):\n",
    "        print(\"model loaded!\")\n",
    "        checkpoint = torch.load(\"save.pt\")\n",
    "        model1.load_state_dict(checkpoint[\"model1\"])\n",
    "        model2.load_state_dict(checkpoint[\"model2\"])\n",
    "\n",
    "    if not is_train:\n",
    "        model1.eval()\n",
    "        model2.eval()\n",
    "\n",
    "    for i in itertools.count():\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            for t in range(HORIZON):\n",
    "                def get_action(model):\n",
    "                    pi = model.pi(torch.from_numpy(s).float())\n",
    "#                     if is_train:\n",
    "                    a = pi.sample()\n",
    "#                     else:\n",
    "#                         a = torch.argmax(pi.probs)\n",
    "                    return a.item(), pi.probs[a].item()\n",
    "                \n",
    "                a1, prob1 = get_action(model1)\n",
    "                a2, prob2 = get_action(model2)\n",
    "                \n",
    "                if t % 126 == 0:\n",
    "                    env.step1(a1)\n",
    "                next_s, r, done, info = env.step2(a2)\n",
    "                if t % 126 == 0:\n",
    "                    model1.store((s, a1, r, next_s, prob1, done))\n",
    "                model2.store((s, a2, r, next_s, prob2, done))\n",
    "\n",
    "                s = next_s\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            model1.update()\n",
    "            model2.update()\n",
    "\n",
    "        if not is_train:\n",
    "            env.save_csv()\n",
    "            break\n",
    "\n",
    "        if i%save_interval==0 and i!=0:\n",
    "            torch.save({\"model1\": model1.state_dict(),\n",
    "                        \"model2\": model2.state_dict()}, f\"save_{i}.pt\")\n",
    "            if killer.kill_now:\n",
    "                if input('Terminate training (y/[n])? ') == 'y':\n",
    "                    env.save_csv()\n",
    "                    break\n",
    "                killer.kill_now = False\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
